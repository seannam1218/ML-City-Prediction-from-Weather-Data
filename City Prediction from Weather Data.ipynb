{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We have a weather dataset of 26 North American cities from roughly 1960 to 2015, downloaded from the Global Climatology Network. The dataset consists of the following monthly weather data for each city for each year: \n",
    "    1. average maximum temperature(0.1°C)\n",
    "    2. average minimum temperature(0.1°C)\n",
    "    3. average precipitation(0.1mm)\n",
    "    4. average snowfall(mm)\n",
    "    5. average snow depth(mm)\n",
    "    \n",
    "Using Python and relevant data science libraries, We plan to build and train an accurate machine learning algorithm to classify given unlabelled weather data into their corresponding North American cities. We will try a few different algorithms, validate them, and then choose the one with the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n",
    "\n",
    "We need to import Python's data science libraries and read the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>tmax-01</th>\n",
       "      <th>tmax-02</th>\n",
       "      <th>tmax-03</th>\n",
       "      <th>tmax-04</th>\n",
       "      <th>tmax-05</th>\n",
       "      <th>tmax-06</th>\n",
       "      <th>tmax-07</th>\n",
       "      <th>tmax-08</th>\n",
       "      <th>...</th>\n",
       "      <th>snwd-03</th>\n",
       "      <th>snwd-04</th>\n",
       "      <th>snwd-05</th>\n",
       "      <th>snwd-06</th>\n",
       "      <th>snwd-07</th>\n",
       "      <th>snwd-08</th>\n",
       "      <th>snwd-09</th>\n",
       "      <th>snwd-10</th>\n",
       "      <th>snwd-11</th>\n",
       "      <th>snwd-12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anchorage</td>\n",
       "      <td>1960</td>\n",
       "      <td>-46.516129</td>\n",
       "      <td>-9.482759</td>\n",
       "      <td>-9.677419</td>\n",
       "      <td>52.400000</td>\n",
       "      <td>140.967742</td>\n",
       "      <td>173.166667</td>\n",
       "      <td>180.225806</td>\n",
       "      <td>168.064516</td>\n",
       "      <td>...</td>\n",
       "      <td>290.903226</td>\n",
       "      <td>44.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.433333</td>\n",
       "      <td>77.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anchorage</td>\n",
       "      <td>1961</td>\n",
       "      <td>-26.096774</td>\n",
       "      <td>-44.571429</td>\n",
       "      <td>-35.064516</td>\n",
       "      <td>58.200000</td>\n",
       "      <td>140.193548</td>\n",
       "      <td>169.633333</td>\n",
       "      <td>178.645161</td>\n",
       "      <td>161.806452</td>\n",
       "      <td>...</td>\n",
       "      <td>113.096774</td>\n",
       "      <td>8.433333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.032258</td>\n",
       "      <td>98.366667</td>\n",
       "      <td>147.258065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anchorage</td>\n",
       "      <td>1962</td>\n",
       "      <td>-59.225806</td>\n",
       "      <td>-31.750000</td>\n",
       "      <td>-18.903226</td>\n",
       "      <td>69.366667</td>\n",
       "      <td>111.419355</td>\n",
       "      <td>159.633333</td>\n",
       "      <td>187.451613</td>\n",
       "      <td>176.483871</td>\n",
       "      <td>...</td>\n",
       "      <td>128.645161</td>\n",
       "      <td>5.866667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>46.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anchorage</td>\n",
       "      <td>1963</td>\n",
       "      <td>-39.290323</td>\n",
       "      <td>-11.357143</td>\n",
       "      <td>-1.451613</td>\n",
       "      <td>41.700000</td>\n",
       "      <td>134.258065</td>\n",
       "      <td>146.200000</td>\n",
       "      <td>185.612903</td>\n",
       "      <td>182.129032</td>\n",
       "      <td>...</td>\n",
       "      <td>60.645161</td>\n",
       "      <td>78.766667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.161290</td>\n",
       "      <td>34.466667</td>\n",
       "      <td>18.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anchorage</td>\n",
       "      <td>1964</td>\n",
       "      <td>-59.129032</td>\n",
       "      <td>-24.655172</td>\n",
       "      <td>-35.096774</td>\n",
       "      <td>45.866667</td>\n",
       "      <td>99.903226</td>\n",
       "      <td>173.566667</td>\n",
       "      <td>182.516129</td>\n",
       "      <td>163.483871</td>\n",
       "      <td>...</td>\n",
       "      <td>114.793103</td>\n",
       "      <td>53.266667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.516129</td>\n",
       "      <td>148.133333</td>\n",
       "      <td>345.870968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        city  year    tmax-01    tmax-02    tmax-03    tmax-04     tmax-05  \\\n",
       "0  Anchorage  1960 -46.516129  -9.482759  -9.677419  52.400000  140.967742   \n",
       "1  Anchorage  1961 -26.096774 -44.571429 -35.064516  58.200000  140.193548   \n",
       "2  Anchorage  1962 -59.225806 -31.750000 -18.903226  69.366667  111.419355   \n",
       "3  Anchorage  1963 -39.290323 -11.357143  -1.451613  41.700000  134.258065   \n",
       "4  Anchorage  1964 -59.129032 -24.655172 -35.096774  45.866667   99.903226   \n",
       "\n",
       "      tmax-06     tmax-07     tmax-08  ...     snwd-03    snwd-04  snwd-05  \\\n",
       "0  173.166667  180.225806  168.064516  ...  290.903226  44.066667      0.0   \n",
       "1  169.633333  178.645161  161.806452  ...  113.096774   8.433333      0.0   \n",
       "2  159.633333  187.451613  176.483871  ...  128.645161   5.866667      0.0   \n",
       "3  146.200000  185.612903  182.129032  ...   60.645161  78.766667      0.0   \n",
       "4  173.566667  182.516129  163.483871  ...  114.793103  53.266667      0.0   \n",
       "\n",
       "   snwd-06  snwd-07  snwd-08  snwd-09    snwd-10     snwd-11     snwd-12  \n",
       "0      0.0      0.0      0.0      0.0   0.000000   29.433333   77.612903  \n",
       "1      0.0      0.0      0.0      0.0  45.032258   98.366667  147.258065  \n",
       "2      0.0      0.0      0.0      0.0   0.000000   10.000000   46.548387  \n",
       "3      0.0      0.0      0.0      0.0   8.161290   34.466667   18.032258  \n",
       "4      0.0      0.0      0.0      0.0  15.516129  148.133333  345.870968  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('monthly-data-labelled.csv')\n",
    "#This is what the data looks like\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this dataframe, we need a two-dimensional array consisting of weather data values without the city label, and another array consisting of the corresponding city labels. Machine learning models are trained using this format of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = data.drop('city', 1).values #remove 'city' column\n",
    "y = data['city'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validating\n",
    "\n",
    "We will try three types of machine learning models: Naive Bayesian, k-Nearest Neighbours, and Support Vector Machine. We will create pipelines to scale each feature's values to lie between 0 and 1 before computing the classifications.\n",
    "\n",
    "For each model, we will divide the data into training and validation sets. This is how we will find out the accuracy of the trained model - by testing the model on the validation set, which will have never been shown to the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bayes model score: 0.6514942528735633\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "n = 30 #number of iterations\n",
    "total = 0\n",
    "for i in range(n):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "    bayes_model = make_pipeline(\n",
    "        MinMaxScaler(),\n",
    "        GaussianNB()\n",
    "        )\n",
    "    bayes_model.fit(X_train, y_train) \n",
    "    total += bayes_model.score(X_valid, y_valid)\n",
    "\n",
    "print(\"bayes model score: \" + str(total/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bayes model has a score of roughly 65% accuracy, which is terrible. Why is this the case? NB algorithm assumes input variables to be independent, and also assumes input data to be distributed normally. None of these assumptions would hold true for weather data. For example, temperature is highly correlated with snowfall and snow depth. Moreoever, monthly temperatures throughout the decades have no reason to be normally distributed. This explains the model's low score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn model score: 0.6647126436781612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_nbs = 5 #numer of neighbours to consider\n",
    "n = 30 #number of iterations\n",
    "total = 0\n",
    "for i in range(n):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "    knn_model = make_pipeline(\n",
    "        MinMaxScaler(),\n",
    "        KNeighborsClassifier(n_neighbors=n_nbs)\n",
    "        )\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    total += knn_model.score(X_valid, y_valid)\n",
    "\n",
    "print(\"knn model score: \" + str(total/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is around 65%, which is as bad as the NB approach. One guess as to why this algorithm is not effective is because we have mapped the values of all the features to the same scale. Since kNN measures the Euclidian distance between data points in the feature space to determine the class of the prediction input, scaling as mentioned results in all the features contributing approximately the same weight to the distance calcuation. We have no knowledge of which features most significantly affect classification, and it could be the case that some features should be more heavily weighed than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc model score: 0.8137931034482758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "n = 30 #number of iterations\n",
    "total = 0\n",
    "for i in range(n):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "    svc_model = make_pipeline(\n",
    "        MinMaxScaler(),\n",
    "        SVC(kernel='rbf', C=5, gamma='scale', decision_function_shape='ovr')\n",
    "        )\n",
    "    svc_model.fit(X_train, y_train)\n",
    "    total += svc_model.score(X_valid, y_valid)\n",
    "\n",
    "print(\"svc model score: \" + str(svc_model.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our SVM model's accuracy is around 80% - much better than the other two. SVM draws optimal hyperplanes between classes, which is more flexible and accurate than the methods of the other two machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We trained three machine learning models (Naive Bayes, k-Nearest Neighbours, and Support Vector Machine) using a relatively small decades-long weather dataset with 60 features (5 weather measurements * 12 months). The models were trained to predict cities from weather data, then were evaluated for their accuracy. The SVM model had the top accuracy of around 80%, while the other two models had an accuracy of around 66%.\n",
    "\n",
    "Our SVM model is far from being practically useful. The model can be improved by adding more rows; cities like Denver has fewer data points than the rest of the cities. Perhaps Denver's (and other cities') missing weather information can be researched and added to the table. Another way to improve the model is to add more relevant features to our data. Adding wind velocity and atmospheric pressure, for example, will significantly improve all our models' accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
